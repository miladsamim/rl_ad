Environment: CarRacing1670756999.738431
seed: None
actions: {0: [-1, 0, 0], 1: [1, 0, 0], 2: [0, 1, 0], 3: [0, 0, 0.8], 4: [0, 0, 0]}
Architecture: DriveDQN_simple_fusion(
  (sensor_net): SensorModel(
    (cnn): Nature_Paper_Conv_Dropout_Torch(
      (layer1_cnn): Conv2d(3, 32, kernel_size=(8, 8), stride=(4, 4), padding=(2, 2))
      (layer2_cnn): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (layer3_cnn): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)
      (layer_4_dense): Linear(in_features=9216, out_features=512, bias=True)
      (out_layer): Linear(in_features=512, out_features=128, bias=True)
    )
    (steering): Linear(in_features=1, out_features=128, bias=True)
    (speed): Linear(in_features=1, out_features=128, bias=True)
    (gyro): Linear(in_features=1, out_features=128, bias=True)
    (abs1): Linear(in_features=1, out_features=128, bias=True)
    (abs2): Linear(in_features=1, out_features=128, bias=True)
    (abs3): Linear(in_features=1, out_features=128, bias=True)
    (abs4): Linear(in_features=1, out_features=128, bias=True)
  )
  (temporal_net): Transformer(
    (encoder): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
          )
          (linear1): Linear(in_features=128, out_features=512, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=512, out_features=128, bias=True)
          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
        (1): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
          )
          (linear1): Linear(in_features=128, out_features=512, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=512, out_features=128, bias=True)
          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
      )
      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
    (decoder): TransformerDecoder(
      (layers): ModuleList(
        (0): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
          )
          (linear1): Linear(in_features=128, out_features=512, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=512, out_features=128, bias=True)
          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
          (dropout3): Dropout(p=0.1, inplace=False)
        )
      )
      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
  )
  (positional_encoder): PositionalEncoding(
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (action_emb): Embedding(5, 128)
  (out): Linear(in_features=128, out_features=1, bias=True)
  (cnn_1d_sensor): Conv1d(8, 8, kernel_size=(8,), stride=(4,), groups=8)
  (merge_sensors): Linear(in_features=376, out_features=128, bias=True)
)
Explore Rate: max(0.1, (1 - float(training_metadata.frame) / training_metadata.frame_limit))
Learning Rate: 0.00025
Discount: 0.99
Batch Size: 32
Memory Capacity: 15000
Num Episodes: 3000
Learning Rate Drop Frame Limit: 250000
